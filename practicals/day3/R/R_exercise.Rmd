---
title: "Classification Algorithms in R"
author: "UU Methodology and Statistics"
params:
  answers: true
output: 
   html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    number_sections: false
---

<style type="text/css">
  
body{ /* Normal  */
  font-size: 12px;
  }
td {  /* Table  */
  font-size: 12px;
}
h1.title {
  font-size: 18px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 18px;
}
h2 { /* Header 2 */
  font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 18px;
}
code.r{ /* Code block */
  font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
  font-size: 14px;
}
</style>

---

# Introduction

---

In this practical, we will focus on many multiple data sets and some popular classification methods: 

- Credit card `Default`: KNN-classification and logistic regression
- Survival in the `titanic` disaster: Support Vector Machines
- Pima Indians Diabetes data: Tree-based models. 

You can use the `Table of Contents` to the right to quickly navigate through this document. 

---

One of the packages we will use is `class`. For this, you will probably need to `install.packages("class")` before running the `library()` functions. `ISLR` is also a new package, that needs to be installed to access the `Default` data. This may hold for more packages that are defined below. I'd like to specifically mention [the `caret` package](https://topepo.github.io/caret/), which brings together the state-of-the-art in statistical learning applications across `R`. It is a wonderful package to run, compare and evaluate machine learning models in `R`. 

```{r packages, warning = FALSE, message = FALSE}
library(MASS)
library(tidyverse)
library(magrittr)
library(class)
library(ISLR)
library(tidyverse)
library(caret)
library(kernlab)
library(MLeval)
library(pROC)
```

Before starting with the exercises, it is a good idea to set your seed, so that (1) your answers are reproducible and (2) you can compare your answers with the answers provided. 

```{r seed1}
set.seed(45)
```

---

# Part A: Default data

---

The `Default` data set (from package `ISLR`) contains credit card loan data for 10.000 people. The goal is to classify credit card cases as `yes` or `no` based on whether they will default on their loan.

---

A1. __Create a scatterplot of the `Default` dataset, where `balance` is mapped to the x position, `income` is mapped to the y position, and `default` is mapped to the colour. Can you see any interesting patterns already?__ 

---

```{r defaultplot1, include = params$answers}
ISLR::Default %>% 
  arrange(default) %>% # so the yellow dots are plotted after the blue ones
  ggplot(aes(x = balance, y = income, colour = default)) +
  geom_point(size = 1.3) +
  theme_minimal()
```

It is clear that people with a larger remaining balance are more likely to default. When you look carefully, you may be able to identify two clusters in the data. One cluster for lower incomes and one cluster for higher incomes. The probability to default follows the same pattern in both clusters: higher remaining balance means an increase in probability to default. 


---

A2. __Add the line `+ facet_grid(cols = vars(student))` to the plot. What do you see?__

---


```{r defaultplot2}
Default %>% 
  arrange(default) %>% # so the yellow dots are plotted after the blue ones
  ggplot(aes(x = balance, y = income, colour = default)) +
  geom_point(size = 1.3) +
  theme_minimal() +
  facet_grid(cols = vars(student))
```


Clearly, if we facet the plot over students and non-students, we see that the lower income group is well-represented by the students. 

---

A3. __Transform "student" into a dummy variable using `ifelse()` (0 = not a student, 1 = student). Then, randomly split the Default dataset into a training set `train` (80%) and a test set `test` (20%).__

---

```{r split, include = params$answers}
# Create train/test split
trainIndex <- createDataPartition(Default$default, p = .8, list = FALSE)

Default %<>% 
  mutate(student = ifelse(student == "Yes", 1, 0)) 
  
train <- Default[trainIndex, ]
test <- Default[-trainIndex, ]
```

The above code splits the data into two parts:

1. A training part that contains 80% of the cases
2. A test part that contains the remaining 20% of the cases. 

These parts are stored in the `train` and `test` objects, respectively. The goal of this splitting is to identify the fit of the model by not using all cases to train the model on. If we would train the model on the same data that we would evaluate the model on, we run the risk of overfitting our model and fovouring our evaluations. By holding out a part of the data we can test if our model indeed fits on cases that have never been *seen by the model* before. 

---

## K-Nearest Neighbours

---

Now that we have explored the dataset, we can start on the task of classification. We can imagine a credit card company wanting to predict whether a customer will default on the loan so they can take steps to prevent this from happening.

The first method we will be using is k-nearest neighbours (KNN). It classifies datapoints based on a majority vote of the k points closest to it. In `R`, the `class` package contains a `knn()` function to perform knn.

---

A4. __Create class predictions for the test set using the `knn()` function. Use `student`, `balance`, and `income` (but no basis functions of those variables) in the `train` dataset. Set `k` to 5. Store the predictions in a variable called `knn_5_pred`.__

---


```{r knn5, include = params$answers}
knn_5_pred <- knn(
  train = train %>% select(-default),
  test  = test  %>% select(-default),
  cl    = as_factor(train$default),
  k     = 5
)
```
The `knn_5_pred` object contains the predictions for the `test` data column `default`. These predictions are obtained by applying the `knn()` function on the `train` set. To be able to generate predictions, the `knn()` has been given the true classes/values (`cl`) from the training data set. The `knn()` function has not seen the true classes for the `test set. 

---

A5. __Create two scatter plots with income and balance as in the first plot you made. One with the true class (`default`) mapped to the colour aesthetic, and one with the predicted class (`knn_5_pred`) mapped to the colour aesthetic.__

_Hint: Add the predicted class `knn_5_pred` to the `test` dataset before starting your `ggplot()` call of the second plot. What do you see?_

---

```{r plotknn, results = "hold", include = params$answers}
# first plot is the same as before
test %>% 
  arrange(default) %>% 
  ggplot(aes(x = balance, y = income, colour = default)) +
  geom_point(size = 1.3) + 
  theme_minimal() +
  labs(title = "True class")

# second plot maps pred to colour
bind_cols(test, default_pred = knn_5_pred) %>% 
  arrange(default) %>% 
  ggplot(aes(x = balance, y = income, colour = default_pred)) +
  geom_point(size = 1.3) + 
  theme_minimal() +
  labs(title = "Predicted class (5nn)")
```

From these plots it is clear that there are quite some misclassifications. Many `No` predictions with `Yes` as true values, and vice versa. 

---

A6. __Repeat the same steps, but now with a `knn_2_pred` vector generated from a 2-nearest neighbours algorithm. Are there any differences?__

---

```{r knn2, include = params$answers}
knn_2_pred <- knn(
  train = train %>% select(-default),
  test  = test  %>% select(-default),
  cl    = train$default,
  k     = 2
)

# repeat the second plot from the previous exercise on the new knn predictions
bind_cols(test, default_pred = knn_2_pred) %>% 
  arrange(default) %>% 
  ggplot(aes(x = balance, y = income, colour = default_pred)) +
  geom_point(size = 1.3) + 
  theme_minimal() +
  labs(title = "Predicted class (2nn)")
```

Compared to the KNN (K=5) model, more people get classified as `Yes`. Still, the KNN (K=2) model is far from perfect. t

---

## Confusion matrix

---

The confusion matrix is an insightful summary of the plots we have made and the correct and incorrect classifications therein. A confusion matrix can be made in `R` with the `confusionMatrix()` function from the `caret` package. 

```{r confmat1}
confusionMatrix(knn_2_pred, test$default)
```

The confusion matrix is an insightful means of studying the performance of a classification model. By looking at the crosstable of predictions against observations, we can study the rates by which our fitted model results in correct and false predictions. In this case, our model results in 95.3% correct predictions on the credit card default data. However, the baseline accuracy (prevalence) of this data set is 96.7%. In other words, if we would simply have predicted our data as `No Default`, we would have gotten a higher accuracy. This places the accuracy for this model in another light. Just like we said in the lecture, don't stare blind on accuracy as it is not a good measure of performance in unbalanced data. 

---

A7. __What would this confusion matrix look like if the classification were perfect?__

---

If the classification would be perfect, the confusion matrix would be:
```{r confmatb, echo = FALSE}
confusionMatrix(test$default, test$default)
```

---

A8. __Make a confusion matrix for the 5-nn model and compare it to that of the 2-nn model. What do you conclude?__

---

```{r confmat3, include = params$answers}
confusionMatrix(knn_5_pred, test$default)
```
The KNN (K=2) model has more true positives (`yes`-`yes`) but also more false
positives (truly `No` in the Reference but predicted `Yes`). Overall the KNN (K=5) model has slightly better accuracy (proportion of correct classifications). However, although this accuracy is higher than if we would randomly assign `Yes` or `No` to cases, the performance of the model is identical to the performance when we would hava classified all values as `No`. 

---

## Logistic regression

---

KNN directly predicts the class of a new observation using a majority vote of the existing observations closest to it. In contrast to this, logistic regression predicts the `log-odds` of belonging to category 1. These log-odds can then be transformed to probabilities by performing an inverse logit transform:

$$ p = \frac{1}{1+e^{-\alpha}}$$, where $\alpha$ indicates log-odds for being in class 1 and $p$ is the probability.

Therefore, logistic regression is a `probabilistic` classifier as opposed to a `direct` classifier such as KNN: indirectly, it outputs a probability which can then be used in conjunction with a cutoff (usually 0.5) to classify new observations.

Logistic regression in `R` happens with the `glm()` function, which stands for generalized linear model. Here we have to indicate that the residuals are modeled not as a gaussian (normal distribution), but as a `binomial` distribution.

--- 

A9. __Use `glm()` with argument `family = binomial` to fit a logistic regression model `fit` to the `train` data.__

---

```{r lrmod, include = params$answers}
fit <- glm(default ~ ., family = binomial, data = train)
```

Now we have generated a model, we can use the `predict()` method to output the estimated probabilities for each point in the training dataset. By default `predict` outputs the log-odds, but we can transform it back using the inverse logit function of before or setting the argument `type = "response"` within the predict function. 

---

A10. __Visualise the predicted probabilities versus observed class for the training dataset in `fit`. You can choose for yourself which type of visualisation you would like to make. Write down your interpretations along with your plot.__

---


```{r visdif, include = params$answers}

tibble(observed  = train$default, 
       predicted = predict(fit, type = "response")) %>% 
  ggplot(aes(y = predicted, x = observed, colour = observed)) +
  geom_point(position = position_jitter(width = 0.2), alpha = .3) +
  scale_colour_manual(values = c("dark blue", "orange"), guide = "none") +
  theme_minimal() +
  labs(y = "Predicted probability to default")
```

This plot shows the predicted probabilities (obtained with `predict(fit, type = "response")`) for all the points in the `test` set. We can see that the defaulting (`Yes`) category has a higher average probability for a default compared to the `No` category, but there are still data points in the `No` category with high predicted probability for defaulting. 

One advantage of parametric procedures like logistic regression is that we get parameters (coefficients) we can interpret.

---

A11. __Look at the coefficients of the `fit` model and interpret the coefficient for `balance`. What would the probability of default be for a person who is not a student, has an income of 40000, and a balance of 3000 dollars at the end of each month? Is this what you expect based on the plots we've made before?__

---

```{r coefs, include = params$answers}

coefs <- coef(fit)
coefs["balance"]
```
The higher the `balance`, the higher the log-odds of defaulting. To be more precise: each dollar increase in `balance` increases the log-odds of defaulting by 0.0057
Let's study all coefficients.
```{r}
coefs
```
Now, if we would like to calculate the predicted logodds of default for a person who is not a student (0 times the coefficient for `student`), has an income of 40000 (40000 multiplied with the coefficient for `income`) and a balance of 3000 dollars (3000 multiplied with the coefficient for `balance`), we can do the following to calculate the logodds directly:
```{r}
logodds <- coefs[1] + 0*coefs[2] + 40000*coefs[4] + 3000*coefs[3]
```
We can then convert the logodds to a probability by
```{r}
1 / (1 + exp(-logodds))
```
or as 
```{r}
plogis(logodds)
```

There is a probability of .999 of defaulting. This is in line with the plots we have seen before. this new data point would be all the way to the right.

---

## Visualising the effect of the balance variable

In two steps, we will visualise the effect `balance` has on the predicted default probability.

---

A12. __Create a data frame called `balance_df` with 3 columns and 500 rows: `student` always 0, `balance` ranging from 0 to 3000, and `income` always the mean income in the `train` dataset.__

---

```{r marbal, include = params$answers}

balance_df <- tibble(
  student = rep(0, 500),
  balance = seq(0, 3000, length.out = 500),
  income  = rep(mean(train$income), 500)
)

```

---

A13. __Use this dataset as the `newdata` in a `predict()` call using `fit` to output the predicted probabilities for different values of `balance`. Then create a plot with the `balance_df$balance` variable mapped to x and the predicted probabilities mapped to y. Is this in line with what you expect?__

---

```{r marplot, include = params$answers}
balance_df$predprob <- predict(fit, 
                               newdata = balance_df, 
                               type = "response")

balance_df %>% 
  ggplot(aes(x = balance, y = predprob)) +
  geom_line(col = "dark blue", size = 1) +
  theme_minimal()
```

Just before a `balance` of 2000 dollars in the first plot is where the ratio of defaults vs non-defaults is 50/50. This line is exactly what we would expect. 

---

A14. __Create a confusion matrix just as the one for the KNN models by using a cutoff predicted probability of 0.5. Does logistic regression perform better?__

---

```{r confmatlogreg, include = params$answers}

pred_prob <- predict(fit, newdata = test, type = "response")
pred_lr   <- factor(pred_prob > .5, labels = c("No", "Yes"))

confusionMatrix(pred_lr, test$default)
```

Logistic regression performs better than KNN in every way - at least for our model on this data. Remember that we started with a random seed. Every procedure that uses random numbers thereafter has become seed dependent. This also holds for the `train`/`test` split that we realized. A different random split can therefore yield different results. Cross-validation - which I excluded in this practical - can give you an indication of the variance of this difference. 

---

# Part B: Titanic data 

---

Let's take the `titanic` data that we used before and fit the following four models on a training version (70% of cases) of that data set.

1. A logistic regression model
2. A linear kernel SVM
3. A polynomial kernel SVM
4. A radial kernel SVM

Finally, compare the performance of all 4 techniques on the test version (30% of not yet used cases) of that data set. 

---

## Grab the data set
We can use the following code block to directly load the data in our workspace:
```{r}
con <- url("https://www.gerkovink.com/datasets/titanic.csv")
titanic <- read_csv(con)
```

---

## Prepare the data
We need to take care of some columns that are not well-coded. Let's make all the measurement levels as they are supposed to be. That means factors into factors, ordered factors into ordered factors, etc. 
```{r}
titanic %<>% 
  mutate(Pclass   = factor(Pclass, 
                         ordered = TRUE, 
                         labels = c("1st class", "2nd class", "3rd class")), 
         Survived = factor(Survived, 
                           labels = c("Died", "Survived")))

str(titanic)
```
The `%<>%` pipe returns the result of the pipe to the object. 

---

## Validation set
Let's split the titanic data into a training and validation set. Before we do so, we fix the random number generator seed in order to allow for reproduction of our results. Any seed value will do. My favorite seed is `123`.
```{r}
set.seed(123)
```
Now we can split the data into a `test` and a `training` part. 
```{r}
idx <- createDataPartition(titanic$Survived, p = .7, list = FALSE)
train <- titanic[idx, ]
test <- titanic[-idx, ]
```


--- 

## Modeling

We now go through the four models where we predict `Survived` from the other features in `titanic` - with the exception of `Name`, naturally. If we would use `Name`, we would fit a zero-residual model: i.e. a model for every row seperately.

For ease of coding we exclude the `Name` column from the `titanic` set. 
```{r}
train %<>% select(-Name)
```
Again, we use the `%<>%` pipe because it returns the result of the pipe to the object. 

### Logistic regression model
Let's fit the logistic regression model
```{r}
lm.train <- glm(Survived ~ ., 
                data = train, 
                family = binomial(link = "logit"))
```
And generate the predicted values
```{r}
lm.pred <- predict(lm.train, 
                   newdata = test %>% select(-Name),
                   type = "response") 
```

To inspect the performance of the final (and only) model:
```{r}
confusionMatrix(ifelse(lm.pred < .5, "Died", "Survived") %>% factor, 
                test$Survived)
```

---

### Linear kernel SVM
Let's train the linear kernel support vector machine
```{r linearSVM, cache = TRUE}
train_control <- trainControl(method="repeatedcv", number=10, repeats=3,
                              savePredictions = TRUE, 
                              classProbs = TRUE, 
                              verboseIter = FALSE)
linearSVM <- train(Survived ~., 
                  data = train, 
                  method = "svmLinear", 
                  trControl = train_control,  
                  preProcess = c("center","scale"),
                  tuneGrid = expand.grid(C = seq(0.1, 10, by = .5)))
```
When we inspect the object we see that the optimal value for $C$ has been trained to be `r linearSVM$bestTune`

Let's inspect the tuning parameters and the cross-validated performance on the training set. 
```{r}
plot(linearSVM)
```
Let's also inspect the ROC curve on the cross-validated data:
```{r}
plots <- evalm(linearSVM, showplots = FALSE, silent = TRUE)
plots$roc
plots$stdres
```
The Receiver Operator Characteristic (ROC) curve shows the trade-off between sensitivity - or true positive rate (TPR) - and specificity: 1 â€“ false positive rate (FPR). Classifiers that give curves closer to the top-left corner indicate a better performance. A random classifier is expected to yield predictions that result in a perfect relation between sensitivity and specificity. The ROC curve will then go along the diagonal (where FPR = TPR). The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.

The ROC does not depend on the class distribution, making it very useful for evaluating classifiers that aim to predict rare events. Rare events are e.g. disease or disasters, where so-called *class balances* are very skewed. Accuracy would then favor classifiers that always predict a negative outcome. 

We can use the area under the ROC curve (AUC) to compare different predictive classifiers. The AUC on the crossvalidated trained model is `.73`.

```{r}
pred.probs <- predict(linearSVM, 
                      newdata = test %>% select(-Name), 
                      type = "prob") 
ROC <- roc(predictor = pred.probs$Survived,
           response = test$Survived, 
           plot = TRUE)
ROC$auc
plot(ROC)
```

Let's generate the predicted values
```{r}
linearSVM.pred <- predict(linearSVM, 
                          newdata = test %>% select(-Name), 
                          type = "raw") 
```

To inspect the performance of the final model on the test set:
```{r}
confusionMatrix(linearSVM.pred, test$Survived)
```

---

### Polynomial kernel SVM
Let's train the polynomial kernel support vector machine
```{r polySVM, cache = TRUE}
polySVM <- train(Survived ~., 
                 data = train, 
                 method = "svmPoly", 
                 trControl = train_control,  
                 preProcess = c("center","scale"),
                 tuneGrid = expand.grid(C = seq(0.25, 2, by = .25),
                                        scale = seq(0.1, .3, by = .1),
                                        degree = c(1:4)))
```

Let's inspect the tuning parameters and the cross-validated performance on the training set. 
```{r}
plot(polySVM)
polySVM
```

Inspect the ROC curve of the predictions
```{r}
pred.probs <- predict(polySVM, 
                      newdata = test %>% select(-Name), 
                      type = "prob") 
ROC <- roc(predictor = pred.probs$Survived,
           response = test$Survived, 
           plot = TRUE)
ROC$auc
plot(ROC)
```

Now we generate the predicted values
```{r}
polySVM.pred <- predict(polySVM, 
                        newdata = test %>% select(-Name), 
                        type = "raw") 
```

To inspect the performance of the final model on the test set:
```{r}
confusionMatrix(polySVM.pred, test$Survived)
```

---

### Radial kernel SVM
Let's train the polynomial kernel support vector machine
```{r radialSVM, cache = TRUE}
radialSVM <- train(Survived~., 
                   data = train, 
                   method = "svmRadial", 
                   trControl = train_control,  
                   preProcess = c("center","scale"),
                   tuneLength = 10)
```
Instead of specifying a grid, we can also ask `caret` to utilize a tunelength of `10`. It will then cycle over the hyperparameter grid conform this length. For the linear SVM kernel, there is only tuning parameter $C$; `tunelength` needs more than one tuning parameter to be used. When we inspect the object we see that the optimal value for $C$ has been trained to be `r polySVM$bestTune`

When we inspect the object we see that the optimal value for $C$ has been trained to be `r radialSVM$bestTune`

Let's inspect the tuning parameters and the cross-validated performance on the training set. 
```{r}
plot(radialSVM)
radialSVM
```

Let's inspect the ROC curve on the predictions
```{r}
pred.probs <- predict(radialSVM, 
                      newdata = test %>% select(-Name), 
                      type = "prob") 
ROC <- roc(predictor = pred.probs$Survived,
           response = test$Survived, 
           plot = TRUE)
ROC$auc
plot(ROC)
```

And generate the predicted values
```{r}
radialSVM.pred <- predict(radialSVM, 
                          newdata = test %>% select(-Name), 
                          type = "raw") 
```

To inspect the performance of the final model on the test set:
```{r}
confusionMatrix(radialSVM.pred, test$Survived)
```

---

# Part C: Indian Liver Patient data

In this part, we will learn how to use different ensemble methods in `R`, recap on how to evaluate the performance of the methods, and learn how we can substantively interpret the model output.

There are some packages we need for this part of the practical that we did not yet load:
We use the following packages:
```{r packages2, warning = FALSE, message = FALSE}
library(psych)
library(gbm)
library(xgboost)
library(data.table)
library(ggforce)
```

In this practical we will work with the ILPD (Indian Liver Patient Dataset) from the UCI Machine Learning Repository (you can find the data [here](https://archive.ics.uci.edu/ml/datasets/ILPD+(Indian+Liver+Patient+Dataset))). This data set contains data on 414 liver disease patients, and 165 non-patients. In general, medical researchers have two distinct goals when doing research: (1) to be able to classify people in their waiting room as either patients or non-patients, and (2) get insight into the factors that are associated with the disease. In this practical part, we will look at both aspects. 

I have prepared the training and test data sets for you. You can load them in by running the following code block, which grabs the data from one of my repositories. The data are 
```{r}
con <- url("https://www.gerkovink.com/datasets/train_test.Rdata")
load(con)
```

We will use these data sets to make inferences and to train a prediction model.

Before we continue, we fix the random number generator seed. 

```{r seed2}
set.seed(123)
```

---

## Exploring the data

---

__C1. Get an impression of the training data by looking at the structure of the data and creating some descriptive statistics.__

First we inspect the `head()` and `tail()` of the `train` data
```{r explore-data, include = TRUE}
head(train)
tail(train)
```

We can also obtain descriptive statistics about this data as follows
```{r}
train %>%
  select(-c(Gender, Disease)) %>%
  describeBy(train$Disease, fast = TRUE)
```
It is quite clear that there are substantial differences between the diseased and non-diseased in the data.

---

__C2. To further explore the data for this practical, create some interesting data visualizations that show whether there are interesting patterns in the data.__

*Hint:* Think about adding a color aesthetic for the variable `Disease`.

I give here a set of visualization that I think are informative. There are many more visualization that one could create:
```{r exploratory-viz, include = TRUE, cache = TRUE}
train %>%
  select(-Gender) %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = Disease, fill = Disease)) +
  geom_boxplot(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()

train %>%
  select(-Gender) %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = Disease, fill = Disease)) +
  geom_density(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()


prop.table(table(train$Gender, train$Disease), margin = 1) %>%
  as.data.frame %>%
  select(Gender = Var1, Disease = Var2, `Relative Frequency` = Freq) %>%
  ggplot(aes(y = `Relative Frequency`, x = Gender, col = Disease, fill = Disease)) +
  geom_histogram(alpha = 0.8, stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "Paired") +
  scale_color_brewer(palette = "Paired") +
  theme_minimal()
```
From these visualizations we can see differences between the distributions for the two `Disease` categories. However, these differences do not seem to be dramatic. Additionally, there are relatively more women with the liver disease than men.

---

## Boosting vs Bagging vs Random Forests

---

__C3. Shortly reflect on the difference between bagging, random forests, and boosting.__

```{r ensemble-methods, include = TRUE}
## Bagging:       fit a regression tree to N bootstrap samples of the training data
##                take the average of all classification trees to base predictions on
##                Note: out-of-bag data can serve as internal validation set.

## Random forest: Similarly to bagging, classification trees are trained on 
##                a bootstrap sample of the data. However, the decision trees
##                are trained using a subset of features from the data. 

## Boosting:      We build a decision tree sequentially. Given the current
##                we fit a (small) tree on the residuals of the current model, 
##                rather than on the outcome Y
```

---

We are going to apply different machine learning models using the `caret` package. 

---

## Bagging

---

__C4. Apply bagging to the training data, to predict the outcome `Disease`, using the `caret` package.__

*Note.* We first specify the internal validation settings, like so:

```{r cross-validation-settings}
cvcontrol <- trainControl(method = "repeatedcv", 
                          number = 10,
                          allowParallel = TRUE)
```

These settings can be inserted within the `train` function from the `caret` package. Make sure to use the `treebag` method, to specify `cvcontrol` as the `trControl` argument and to set `importance = TRUE`.

```{r train-bag, include = TRUE}
bag_train <- train(Disease ~ .,
                   data = train, 
                   method = 'treebag',
                   trControl = cvcontrol,
                   importance = TRUE)
```


---

### Variable importance and performance

---

__C5. Interpret the variable importance measure using the `varImp` function on the trained model object.__

```{r bag-importance, include = TRUE}
bag_train %>%
  varImp %>%
  plot
```

---

__C6. Create training set predictions based on the bagged model, and use the `confusionMatrix()` function from the `caret` package to assess it's performance.`__

*Hint: You will have to create predictions based on the trained model for the training data, and evaluate these against the observed values of the training data.*

```{r bag-training-predictions, include = TRUE}
confusionMatrix(predict(bag_train, type = "raw"),
                train$Disease)
```
We have realized near-perfect training set performance. However, this shows nothing more than that we have been able to train the model rather well. We need to evaluate our model on the test set before we can draw conclusions about predicive power and test error. 

---

__C7. Now ask for the output of the bagged model. Explain why the under both approaches differ.__

```{r show-bag, include = TRUE}
bag_train
```

---

We will now follow the same approach, but rather than bagging, we will train a random forest on the training data. 

---

## Random Forest

---

__C8. Fit a random forest to the training data to predict the outcome `Disease`, using the `caret` library.__

*Note.* Use the same `cvcontrol` settings as in the previous model.

```{r train-rf, include = TRUE}
rf_train <- train(Disease ~ .,
                  data = train, 
                  method = 'rf',
                  trControl = cvcontrol,
                  importance = TRUE)
```

---

__C9. Again, interpret the variable importance measure using the `varImp` function on the trained model object. Do you draw the same conclusions as under the bagged model?__

```{r rf-importance, include = TRUE}
rf_train %>%
  varImp %>%
  plot
```

The random forest model `rf_train` indicates a different variable importance than the bagged model `bag_train`. This is due to the random selection of predictors within random forests: the bootstrap-based trees are thus decorrelated. 

---

__C10. Output the model output from the random forest. Are we doing better than with the bagged model?__

```{r show-rf, include = TRUE}
rf_train
```
Yes, the most accurate model indicates that we do just slightly better than with the bagged model. However, this might well be due to chance.

---

## Boosting

---

__C11. Now, fit a boosting model using the `caret` library to predict disease status.`__

*Hint:* Use gradient boosting (the `gbm` method in `caret`).

```{r fit-gbm, include = TRUE, message = FALSE}
gbm_train <- train(Disease ~ .,
                   data = train,
                   method = "gbm",
                   verbose = F,
                   trControl = cvcontrol)
```

---

__C12. Again, interpret the variable importance measure. You will have to call for `summary()` on the model object you just created. Compare the output to the previously obtained variable importance measures.__

```{r gbm_importance, include = TRUE}
summary(gbm_train)
```

---

__C13. Output the model output from our gradient boosting procedure. Are we doing better than with the bagged and random forest model?__

```{r show-gbm, include = TRUE}
gbm_train
```
Yes, our best model is doing slightly better then the previous two models. However, the performance gain is small and might be due to random variation.

---

## `xgboost` and `SHAP`

---

For now, we will continue with extreme gradient boosting, although we will use a difference procedure.


We will use `xgboost` to train a binary classification model, and create some visualizations to obtain additional insight in our model. We will create the visualizations using `SHAP` (**SH**apley **A**dditive ex**P**lanations) values, which are a measure of importance of the variables in the model. In fact, `SHAP` values indicate the influence of each input variable on the predicted probability for each person. Essentially, these give an indication of the difference between the predicted probability with and without that variable, for each person's score.

---

__C14. Download the file `shap.R` from [this](https://github.com/pablo14/shap-values) Github repository.__

*Note.* There are multiple ways to this, of which the simplest is to run the following code. 

```{r download-shap-functions, message = FALSE}
con <- url("https://github.com/pablo14/shap-values/blob/master/shap.R?raw=TRUE")
source(con)
```

---

__C15. Specify your model as follows, and use it to create predictions on the training data.__

```{r xgboost, results = FALSE, message = FALSE}
train_x <- model.matrix(Disease ~ ., train)[,-1]
train_y <- as.numeric(train$Disease) - 1
xgboost_train <- xgboost(data = train_x,
                         label = train_y, 
                         max.depth = 10,
                         eta = 1,
                         nthread = 4,
                         nrounds = 4,
                         objective = "binary:logistic",
                         verbose = 2)

pred <- tibble(Disease = predict(xgboost_train, newdata = train_x)) %>%
  mutate(Disease = factor(ifelse(Disease < 0.5, 1, 2),
                          labels = c("Healthy", "Disease")))
confusionMatrix(pred$Disease, train$Disease)
```

---

__C16. First, calculate the `SHAP` rank scores for all variables in the data, and create a variable importance plot using these values. Interpret the plot.__

```{r make-shap, results = TRUE, fig.show='hide'}
shap_results <- shap.score.rank(xgboost_train,
                                X_train = train_x,
                                shap_approx = F)

var_importance(shap_results)
```

---

__C17. Plot the `SHAP` values for every individual for every feature and interpret them.__

```{r shap-plots, results = TRUE}
shap_long <- shap.prep(shap = shap_results,
                       X_train = train_x)

plot.shap.summary(shap_long)

xgb.plot.shap(train_x, features = colnames(train_x), model = xgboost_train, n_col = 3)
```

The first plot demonstrates that those with a high value for
Direct_Bilirubin have a lower probability of being diseased. Also,
Those with a higher age have a lower probability of being diseased,
while those with a higher Albumin have a higher probability of being diseased.

The second set of plots displays the marginal relationships of the SHAP values with the predictors. This conveys the same information, but in greater detail. The interpretability may be a bit tricky for the inexperienced data analyst. 

---

__C18. Verify which of the models you created in this practical performs best on the test data.__

```{r test-models, include = TRUE}
bag_test <- predict(bag_train, newdata = test)
rf_test  <- predict(rf_train, newdata = test)
gbm_test <- predict(gbm_train, newdata = test)
xgb_test <- predict(xgboost_train, newdata = model.matrix(Disease ~ ., test)[,-1]) %>%
  factor(x = ifelse(. < 0.5, 1, 2), levels = c(1,2), labels = c("Healthy", "Disease"))

list(`bagging` = bag_test, 
     `random_forest` = rf_test, 
     `gradient_boosting` = gbm_test, 
     `xtreme_gradient_boosting` = xgb_test) %>%
  map(~ confusionMatrix(.x, test$Disease))
```

The best performing model in this case on this data split is the `gradient boosting` model from exercise `C11`. 

__C19. Verify if the model is calibrated.__
To do so, we take the predicted values from the model in `C11` and bind them together with the observed class for the `test` set. 
```{r}
out <- data.frame(obs = test$Disease,
                  gbm = predict(gbm_train, newdata = test, type = "prob"),
                  rf  = predict(rf_train, newdata = test, type = "prob"),
                  bag = predict(bag_train, newdata = test, type = "prob"),
                  xgb = 1 - predict(xgboost_train, newdata = model.matrix(Disease ~ ., test)[,-1]))
out %>% head()
```
The `caret` package has a function to obtain the calibration data and to plot the calibration plot
```{r}
calibration(obs ~ gbm.Healthy + rf.Healthy + bag.Healthy + xgb, data = out) %>% 
  xyplot( auto.key = list(columns = 2))
```

We can see that for none of the models the probabilities are well-calibrated. 

<!-- One simple way to realize calibration is by running a logistic regression model on the fitted probabilities to model the outcome. Then, the obtained parameters can be used to calibrate the probabilities obtained on the `test` set.  -->

<!-- ```{r} -->
<!-- fitted <- data.frame(obs = train$Disease, -->
<!--                      gbm = predict(gbm_train, type = "prob")) -->
<!-- fitted -->
<!-- fit.glm <- fitted %$%  -->
<!--   glm(obs ~ gbm.Healthy, family = binomial(link="logit")) -->

<!-- pred.glm <- fit.glm %>% predict(newdata = out, type = "response") -->
<!-- ``` -->
<!-- Now we add these calibrated predicted probabilities to the object `out` that we created earlier.  -->
<!-- ```{r} -->
<!-- out$glm <- 1 - pred.glm # to get the probs for Healthy -->
<!-- out -->
<!-- ``` -->
<!-- and we recreate the calibration plot -->
<!-- ```{r} -->
<!-- calibration(obs ~ gbm.Healthy + glm, data = out) %>%  -->
<!--   xyplot( auto.key = list(columns = 2)) -->
<!-- ``` -->


---

End of all exercises